{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing to build a Twitter language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 20000\n",
    "embedding_dimension = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tep.dataLoader import DataLoader\n",
    "dl = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284701"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tweets from file\n",
    "tweets = dl.load_from_file(filename=\"data/tweets_1.json\", ignore_retweets=True)\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "565486"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets += dl.load_from_file(filename=\"data/tweets_2.json\", ignore_retweets=True)\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "781721"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets += dl.load_from_file(filename=\"data/tweets_3.json\", ignore_retweets=True)\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1031077"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets += dl.load_from_file(filename=\"data/tweets_4.json\", ignore_retweets=True)\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1293005"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets += dl.load_from_file(filename=\"data/tweets_5.json\", ignore_retweets=True)\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomize tweet order\n",
    "import random\n",
    "random.seed(1000)\n",
    "random.shuffle(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tep.dataPreprocessor import DataPreprocessor\n",
    "dp = DataPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' <user>  thanks for your support this september !  we appreciate all you do for the kids of <hashtag> st jude ! ',\n",
       " ' <user>   <user>  will not last .  <repeat>',\n",
       " ' <user>   <user>  congrats üëåüèº she is adorable and amazingly chill .  wishing her a beautiful life of joy and success',\n",
       " ' <user>  so sorry to hear the pain you went through after that shooting  -  and hopefully we can figure out how  .  <repeat> <url>',\n",
       " 'is it too much to ask that a <hashtag> healthcare bill actually improve health care ?  we are live <allcaps>  at englewood hospitals <url>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test extracting content\n",
    "texts = dp.extract_content(tweets)\n",
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might need to iterate the following and observe the sorted words file in order to determine a suitable vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=num_words, filters='\"‚Äù‚Äú#$%&()*+,/=@[]^_¬¥`‚Äò{|}~\\t\\n\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205706"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‚Äòdata/lang_model‚Äô: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir data/lang_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word index\n",
    "import json\n",
    "with open('data/lang_model/word_index.json', 'w') as fp:\n",
    "    json.dump(tokenizer.word_index, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unknown>',\n",
       " '.',\n",
       " '<url>',\n",
       " '<user>',\n",
       " 'the',\n",
       " 'to',\n",
       " '<allcaps>',\n",
       " '<hashtag>',\n",
       " '<number>',\n",
       " '<repeat>']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save sorted words\n",
    "import operator\n",
    "sorted_words = sorted(tokenizer.word_index.items(), key=operator.itemgetter(1))\n",
    "sorted_words = [w[0] for w in sorted_words]\n",
    "sorted_words = ['<unknown>'] + sorted_words\n",
    "sorted_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tep.utils import save_as_text\n",
    "save_as_text(sorted_words, 'data/lang_model/word_labels.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sorted_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3,\n",
       "  47,\n",
       "  11,\n",
       "  24,\n",
       "  129,\n",
       "  23,\n",
       "  1997,\n",
       "  12,\n",
       "  19,\n",
       "  371,\n",
       "  58,\n",
       "  15,\n",
       "  43,\n",
       "  11,\n",
       "  4,\n",
       "  398,\n",
       "  13,\n",
       "  7,\n",
       "  312,\n",
       "  2348,\n",
       "  12],\n",
       " [3, 3, 34, 35, 145, 1, 9],\n",
       " [3,\n",
       "  3,\n",
       "  229,\n",
       "  14941,\n",
       "  236,\n",
       "  17,\n",
       "  3876,\n",
       "  16,\n",
       "  12791,\n",
       "  5836,\n",
       "  1,\n",
       "  1284,\n",
       "  165,\n",
       "  10,\n",
       "  709,\n",
       "  188,\n",
       "  13,\n",
       "  2512,\n",
       "  16,\n",
       "  641],\n",
       " [3,\n",
       "  49,\n",
       "  83,\n",
       "  5,\n",
       "  95,\n",
       "  4,\n",
       "  887,\n",
       "  15,\n",
       "  1040,\n",
       "  242,\n",
       "  149,\n",
       "  33,\n",
       "  1278,\n",
       "  20,\n",
       "  16,\n",
       "  2925,\n",
       "  19,\n",
       "  38,\n",
       "  2391,\n",
       "  44,\n",
       "  46,\n",
       "  1,\n",
       "  9,\n",
       "  2],\n",
       " [17,\n",
       "  29,\n",
       "  195,\n",
       "  189,\n",
       "  5,\n",
       "  512,\n",
       "  33,\n",
       "  10,\n",
       "  7,\n",
       "  390,\n",
       "  135,\n",
       "  579,\n",
       "  485,\n",
       "  124,\n",
       "  126,\n",
       "  30,\n",
       "  19,\n",
       "  22,\n",
       "  137,\n",
       "  6,\n",
       "  32,\n",
       "  3325,\n",
       "  2]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = tokenizer.texts_to_sequences(texts)\n",
    "seqs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lens = [len(s) for s in seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90th percentile: 27\n",
      "95th percentile: 29\n",
      "99th percentile: 32\n",
      "100th percentile: 98\n"
     ]
    }
   ],
   "source": [
    "# calculate length percentiles\n",
    "import numpy as np\n",
    "\n",
    "percentiles = [90, 95, 99, 100]\n",
    "for p in percentiles:\n",
    "    print('{}th percentile:'.format(p), int(np.round(np.percentile(seq_lens, p))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 32\n",
    "padded_seqs = pad_sequences(seqs, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tep.utils import save_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(padded_seqs, 'data/lang_model/seqs.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del padded_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create inputs and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs = []\n",
    "for seq in seqs:\n",
    "    for i in range(1, len(seq)):\n",
    "        n_gram_seq = seq[:i+1]\n",
    "        input_seqs.append(n_gram_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "del seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs = np.array(pad_sequences(input_seqs, maxlen=max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22664353, 32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the input sequences\n",
    "random.seed(2018)\n",
    "random.shuffle(input_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = input_seqs[:,:-1], input_seqs[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "del input_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.utils as ku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22664353, 31)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(inputs, 'data/lang_model/inputs.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "del inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22664353,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(labels, 'data/lang_model/labels.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "del labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embeddings index\n",
    "f = open('glove/glove.twitter.27B.200d.txt', encoding='utf-8')\n",
    "embeddings_index = {}\n",
    "for line in f:\n",
    "    vals = line.split()\n",
    "    word = vals[0]\n",
    "    coeffs = np.asarray(vals[1:], dtype='float32')\n",
    "    embeddings_index[word] = coeffs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill embedding matrix\n",
    "embedding_matrix = np.zeros((num_words + 1, embedding_dimension))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    emb_vec = embeddings_index.get(word)\n",
    "    if emb_vec is not None and i <= num_words:\n",
    "        embedding_matrix[i] = emb_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20001, 200)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(embedding_matrix, 'data/lang_model/emb_mat.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_mat.bc  inputs.bc  labels.bc  seqs.bc  word_index.json  word_labels.tsv\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/lang_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
