{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Train feedforward regression model\n",
    "\n",
    "After training a classification model, we now repeat the same process for the regression task. Once again, this notebook serves to prepare automatic testing of several network architectures. Results can be replicated using the corresponding Python script, logs are also stored in the repository.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1293005, 15)\n",
      "(1293005,)\n"
     ]
    }
   ],
   "source": [
    "from tep.utils import load_array\n",
    "feats = load_array(\"data/auxiliary_features.bc\")\n",
    "labels = load_array(\"data/regression_labels.bc\")\n",
    "print(feats.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 120    0    3    5    0    3    0   46    2    0    0    1 2666   74    1\n",
      "    3    0    2    0    0]\n",
      "[ 4.79579055  0.          1.38629436  1.79175947  0.          1.38629436\n",
      "  0.          3.8501476   1.09861229  0.          0.          0.69314718\n",
      "  7.88870952  4.31748811  0.69314718  1.38629436  0.          1.09861229\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# transform labels to log(x+1) in order to account for zero values\n",
    "import numpy as np\n",
    "print(labels[:20])\n",
    "labels = np.log1p(labels)\n",
    "print(labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define configurations to test\n",
    "configs = [\n",
    "    {'name': '1h_16n', 'num_layers': 1, 'num_units': 16},\n",
    "    {'name': '1h_32n', 'num_layers': 1, 'num_units': 32},\n",
    "    {'name': '1h_64n', 'num_layers': 1, 'num_units': 64},\n",
    "    {'name': '2h_16n', 'num_layers': 2, 'num_units': 16},\n",
    "    {'name': '2h_32n', 'num_layers': 2, 'num_units': 32},\n",
    "    {'name': '2h_64n', 'num_layers': 2, 'num_units': 64},\n",
    "    {'name': '3h_16n', 'num_layers': 3, 'num_units': 16},\n",
    "    {'name': '3h_32n', 'num_layers': 3, 'num_units': 32},\n",
    "    {'name': '3h_64n', 'num_layers': 3, 'num_units': 64},\n",
    "    {'name': '4h_16n', 'num_layers': 4, 'num_units': 16},\n",
    "    {'name': '4h_32n', 'num_layers': 4, 'num_units': 32},\n",
    "    {'name': '4h_64n', 'num_layers': 4, 'num_units': 64},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use settings for testing on sample\n",
    "train_size = 10000\n",
    "val_size = 1000\n",
    "batch_size = 64\n",
    "\n",
    "# use settings for running on full data\n",
    "#val_size = 10000\n",
    "#train_size = features.shape[0] - val_size\n",
    "#batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_name = 'dffn_reg'\n",
    "root_path = 'models/' + root_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training model: dffn_reg_1h_16n\n",
      "Training set: 10000 examples\n",
      "Validation set: 1000 examples\n",
      "rm: models/dffn_reg/1h_16n/*: No such file or directory\n",
      "Epoch 00001: val_loss improved from inf to 4.80259, saving model to models/dffn_reg_1h_16n.hdf5\n",
      "Epoch 00002: val_loss improved from 4.80259 to 2.80228, saving model to models/dffn_reg_1h_16n.hdf5\n",
      "Epoch 00003: val_loss improved from 2.80228 to 2.39926, saving model to models/dffn_reg_1h_16n.hdf5\n",
      "Epoch 00004: val_loss improved from 2.39926 to 2.28721, saving model to models/dffn_reg_1h_16n.hdf5\n",
      "Epoch 00005: val_loss improved from 2.28721 to 2.24948, saving model to models/dffn_reg_1h_16n.hdf5\n",
      "Epoch 00006: val_loss improved from 2.24948 to 2.24068, saving model to models/dffn_reg_1h_16n.hdf5\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 00011: val_loss improved from 2.24068 to 2.24025, saving model to models/dffn_reg_1h_16n.hdf5\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 00013: val_loss improved from 2.24025 to 2.18066, saving model to models/dffn_reg_1h_16n.hdf5\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 00023: early stopping\n",
      "loss: 1.9335718418121337, r2: 0.5656791130065918, val_loss: 2.1806572799682615, val_r2: 0.5059107465744018\n",
      "\n",
      "\n",
      "Start training model: dffn_reg_1h_32n\n",
      "Training set: 10000 examples\n",
      "Validation set: 1000 examples\n",
      "rm: models/dffn_reg/1h_32n/*: No such file or directory\n",
      "Epoch 00001: val_loss improved from inf to 3.20452, saving model to models/dffn_reg_1h_32n.hdf5\n",
      "Epoch 00002: val_loss improved from 3.20452 to 2.56647, saving model to models/dffn_reg_1h_32n.hdf5\n",
      "Epoch 00003: val_loss improved from 2.56647 to 2.36142, saving model to models/dffn_reg_1h_32n.hdf5\n",
      "Epoch 00004: val_loss improved from 2.36142 to 2.24684, saving model to models/dffn_reg_1h_32n.hdf5\n",
      "Epoch 00005: val_loss improved from 2.24684 to 2.16264, saving model to models/dffn_reg_1h_32n.hdf5\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 00007: val_loss improved from 2.16264 to 2.05949, saving model to models/dffn_reg_1h_32n.hdf5\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 00017: early stopping\n",
      "loss: 1.9636604209899902, r2: 0.5544910694122315, val_loss: 2.0594907598495484, val_r2: 0.5386983180046081\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tep.deepFeedforwardNetwork import regression_model\n",
    "from tep.modelUtils import save_architecture\n",
    "from tep.trainUtils import get_callbacks, print_regression_metrics, r2\n",
    "\n",
    "for config in configs[:2]:\n",
    "    # clear tf session first in order to avoid conflicts\n",
    "    K.clear_session()\n",
    "\n",
    "    # set model path\n",
    "    model_name = root_name + '_' + config['name']\n",
    "    model_path = root_path + '_' + config['name']\n",
    "    logging_path = root_path + '/' + config['name']\n",
    "    \n",
    "    # Start logging\n",
    "    print(\"Start training model: \" + model_name)\n",
    "    print(\"Training set: {} examples\".format(train_size))\n",
    "    print(\"Validation set: {} examples\".format(val_size))\n",
    "    \n",
    "    # Create logging directory\n",
    "    !mkdir -p $logging_path\n",
    "    # Remove prior logs\n",
    "    !rm $logging_path/*\n",
    "    \n",
    "    # load and save model\n",
    "    model = regression_model(feats.shape[1], config['num_layers'], config['num_units'], metrics=[r2])\n",
    "    save_architecture(model, model_path + '.json')\n",
    "    \n",
    "    # load model callbacks\n",
    "    cbs = get_callbacks(model_name=model_name, log_dir=logging_path, verbose=1)\n",
    "    \n",
    "    # train model\n",
    "    model.fit(feats[:train_size], \n",
    "          labels[:train_size], \n",
    "          validation_data=(feats[-val_size:], labels[-val_size:]), \n",
    "          batch_size=batch_size, \n",
    "          epochs=100, \n",
    "          verbose=0,\n",
    "          shuffle=True,\n",
    "          callbacks=cbs)\n",
    "    \n",
    "    # print best result\n",
    "    history = cbs[2]\n",
    "    print_regression_metrics(history)\n",
    "    \n",
    "    # add newline after model was trained\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze model performance\n",
    "\n",
    "Results from running the training script suggest that the most suitable network architecture contains **5 layers** with **64 units** each. The model showed the following performance on unseen validation data:\n",
    "* **loss: 1.0930**\n",
    "* **R2: 0.7594**\n",
    "\n",
    "Thanks to having saved weights and architecture, we can now load the model from disk and further analyze its regression performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
