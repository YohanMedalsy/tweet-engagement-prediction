{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tep.modelUtils import load_architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/paperspace/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    }
   ],
   "source": [
    "model = load_architecture('models/enhanced_meta/baseline.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text_input (InputLayer)         (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, 32, 100)      72169600    text_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pad_1 (ZeroPadding1D)           (None, 34, 100)      0           word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv1D)                 (None, 32, 64)       19264       pad_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool_1 (MaxPooling1D)           (None, 16, 64)       0           conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "pad_2 (ZeroPadding1D)           (None, 18, 64)       0           pool_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv1D)                 (None, 16, 64)       12352       pad_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool_2 (MaxPooling1D)           (None, 8, 64)        0           conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 8, 64)        0           pool_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "aux_input (InputLayer)          (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_aux (BatchNormalization)     (None, 24)           96          aux_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "comb_input (Concatenate)        (None, 536)          0           flatten[0][0]                    \n",
      "                                                                 bn_aux[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 128)          68736       comb_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bn_1 (BatchNormalization)       (None, 128)          512         fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_2 (Dense)                    (None, 128)          16512       bn_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "bn_2 (BatchNormalization)       (None, 128)          512         fc_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            129         bn_2[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 72,287,713\n",
      "Trainable params: 72,287,153\n",
      "Non-trainable params: 560\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tep.trainUtils import r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='mean_squared_error', metrics=[r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('models/enhanced_meta/baseline.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1293005,)\n",
      "(1293005, 32)\n",
      "(1293005, 24)\n",
      "(721696, 100)\n"
     ]
    }
   ],
   "source": [
    "from tep.utils import load_array\n",
    "y = load_array(\"data/log_labels_v3.bc\")\n",
    "print(y.shape)\n",
    "X_seq = load_array(\"data/sequences_len32_v3.bc\")\n",
    "print(X_seq.shape)\n",
    "X_meta = load_array(\"data/meta_features_v3.bc\")\n",
    "print(X_meta.shape)\n",
    "emb_mat = load_array(\"data/embedding_matrix_100dim_v3.bc\")\n",
    "print(emb_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = X_seq[-10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "word_index_file = open('data/word_index_v3.json')\n",
    "word_index_str = word_index_file.read()\n",
    "word_index = json.loads(word_index_str)\n",
    "type(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unknown>', '.', '<user>', 'to', 'the', '<url>', '!', 'i', '<allcaps>', 'a']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "# sort word index for lookups\n",
    "sorted_word_tuples = sorted(word_index.items(), key=operator.itemgetter(1))\n",
    "sorted_words = ['<unknown>'] + [t[0] for t in sorted_word_tuples]\n",
    "sorted_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word lookup function\n",
    "def lookup_word(word_id):\n",
    "    return sorted_words[word_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# window lookup function\n",
    "def get_words(seq, pos, kernel_size):\n",
    "    pad_size = int(np.floor(np.divide(kernel_size, 2)))\n",
    "    seq = np.pad(seq, pad_size, 'constant', constant_values=0)\n",
    "    seq = seq[pos:(pos + kernel_size)]\n",
    "    words = []\n",
    "    for word_id in seq:\n",
    "        word = lookup_word(word_id)\n",
    "        words.append(word)\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "# activation calculator function\n",
    "def calc_activations(model, layer, batch):\n",
    "    tmp_model = Model(inputs=model.input[0], outputs=model.get_layer(layer).output)\n",
    "    activations = tmp_model.predict(batch)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron interpreter\n",
    "def interpret_neuron(model, batch, layer, neuron, num_results=10):\n",
    "    # get kernel size of conv layer\n",
    "    kernel_size = model.get_layer(layer).kernel_size[0]\n",
    "    # calculate layer activations through forward propagation\n",
    "    activations = calc_activations(model, layer, batch)\n",
    "    # slice in order to retrieve neuron activations\n",
    "    neuron_activations = activations[:,:,neuron]\n",
    "    # sort indices according to hightest activation\n",
    "    sorted_activations = np.argsort(neuron_activations, axis=None)\n",
    "    xs, ys = np.unravel_index(sorted_activations, neuron_activations.shape)\n",
    "    sorted_idx = np.flip(list(zip(xs, ys)), axis=0)[:num_results]\n",
    "    # assemble results object\n",
    "    results = []\n",
    "    for idx in sorted_idx:\n",
    "        result = {\n",
    "            'activation': float(neuron_activations[idx[0], idx[1]]),\n",
    "            'tweet': int(idx[0]),\n",
    "            'position': int(idx[1]),\n",
    "            'words': get_words(batch[idx[0]], idx[1], kernel_size),\n",
    "        }\n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer interpreter\n",
    "def interpret_layer(model, batch, layer, results_per_neuron=10):\n",
    "    result_dict = {}\n",
    "    num_filters = model.get_layer(layer).filters\n",
    "    for i in range(num_filters):\n",
    "        result = interpret_neuron(model, batch, layer, i, num_results=results_per_neuron)\n",
    "        result_dict['Filter_' + str(i)] = result\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretations = interpret_layer(model, X_valid, 'conv_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models/interpretations/’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir models/interpretations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('models/interpretations/baseline_conv1.json', 'w') as f:\n",
    "   json.dump(interpretations, f, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
